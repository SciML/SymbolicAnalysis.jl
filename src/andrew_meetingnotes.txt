So now whatever we talk will get recorded. I just turned everything on.

Okay, so for this point, I can add a description and an example of how if someone wants to add a new atom, they can add it, and how that will lead to a broader set of atoms.

I think that's good. I think that's what they want. They just want to see how we can expand this library or how someone else external to us could help build this library further and what this library could be. They're currently at and how someone can build it further. And then just make it explicit and clear.

Yeah, sounds good. This is something you could just write about since you're coming in. This seems like a paper edition, like a text in the paper.

Okay, clarity of the DGCP paradigm. The concept and significance of the DGCP paradigm itself should be introduced more explicitly, and earlier in the paper. A concise, standalone definition of what constitutes a DGCP problem and why this formulation is powerful would greatly aid readers unfamiliar with this line of research.

This is just pushing how what our definition of DGCP is and why someone should care about this line of research? This goes back to this thing I was talking about when we were writing the paper of having a defined canonical form for our kind of problems.

So maybe this is something we could discuss a bit further later how we should make a concise, very clear, canonical definition of a DGCP problem and then motivate some of the problems. Like why we care? But this is also in the same sense why people care about discipline programming in general. Like, are papers just about extending the notion of discipline convex programming to the geodesically convex case? Yeah, so like but everything. But I guess we need to give a description of what that looks like, right? Like either through equations or in words. I think we have done this in words, but I think we just need to give some more, like, you know, man, I'm going to be super explicit about it, math completely. But like for the canonical DCP problem, for example, there's like C transpose x something like just what I'm doing. Like just one broad description of what these problems look like and in math terms up in the paper, not like deep deep into it.

Okay, okay, no, but that's my read of the Sorry, like just to be clear. I don't know if that's the correct thing. That's my read of what this guy's asking for. Yeah, no, I think you're right. I think we just need to be very clear of what a DGCP problem is. Whether that's through, like, what you said, a canonical representation of it, which I think is fine enough. Or otherwise, but I think canonical representation is probably the best idea. 
Short blurb of why we care about this representation. What gives us further than just convex programming?

Oh yeah, this one I could definitely do. Several atoms appear to be adopted from existing literature to highlight the novel contributions of this work with maximum clarity. Authors should indicate which atoms are introduced here and which are drawn from other work.

I can create this table. Do we have any novel atoms that we have come up with? I think I came up with one or two. Probably won't look very impressive, but yeah, I'm just happy to be considered.

So it does seem like we just need to do the revision. If we address these revisions, it will just pass. What they said was, "Should you prepare to incorporate major revisions satisfying the referee concerns?" Is likely to be eventually accepted.

So I guess the reviewers will still look at the updated one. Obviously, yeah, so there. I think what's going to happen is we're going to submit it, the referees are going to read our revisions again, make sure it's all addressed, addressing their problems, and then if it does, I think they'll be like, "Okay, yeah, sound good, let's move forward."

Numerical evaluations. I get this is the part that they were pretty concerned about. Not concerned but like that they wanted a bigger expansion on. We could get this together. I mean, we can work on this together. We'll have to because there's the what we need to do and then doing it right. So like I'm obviously I can just handle all the doing part but like what we need to do parts need uh like I need collaboration on that. More familiar with the literature and so on.

So yeah, sounds great.

Numerical experiments while demonstrating proof of concept are currently insufficient to fully validate the framework's practical utility and performance. The manuscript would be significantly strengthened by a more comprehensive set of experiments, potentially including comparisons against non-convex baselines or applications to large-scale problems.

Oh okay, so I guess like one concern I would have with that is we don't do any reformulations. Right? Like we don't improve solving in any way. We are just proving that it's either convex or non-convex. So maybe we need to. I had implemented some canonicalization passes. Maybe we pick a problem that starts out non-convex, then we can do a pass of like through this canonicalization based on like whatever rules we kind of know about which will make it convex and then compare those two. Like then solve both versions of that problem, either using the same solver. Hopefully, then the non-convex, like the vanilla problem, takes longer to solve vs like this reformulated nicer version of the problem using our rules takes less time to solve.

Wait sir, I don't understand. So if you have a problem that is non-convex with respect to traditional Euclidean convexity but it's geodesically convex, what they want is saying like how this shift in perspective to geodesic convexity allows better solves. Is that what they're saying? 
One thing is they just say a more comprehensive set of experiments. It's like, it's not very descriptive, but including comparison again, non-convex baseline. So I guess that means like, Oh, like maybe in the Euclidean setting we can't use Newton method because we don't know this is convex and so we have to use some heuristic based method or something which hopefully takes longer to solve. And then because we can show that it's geodesically convex and so on, we can use like the geodesic version of the Newton method and that converges faster than the vanilla non-convex solve. Okay. But that's like, that's just hopeful. I don't know if that will happen. Yeah. I'm just wondering if there's literature already that demonstrates that, right? Like applications to larger scale real world problems to showcase the advantages of automated convexity. Sorry. Yeah. Just wanted to see like what the other part was. Yeah. What do you Yeah, I was, um, I'm just wondering if there's already literature that kind of s- Um, I mean, there's literature from like Melanie's supervisor, uh, Suvrit. Mm-hmm. Yep. About how some problems could be seen as, uh, geodesically convex or around like the square root problem, the like Breskamp-Leeb stuff. Um... Those are all like non-convex, right? Oh yeah. Like the square root of the matrix problem doesn't... isn't that like a large scale real world problem in some ways? Like I wonder if it's being used for something and we can say that oh, like look, we can make this real problem better. Maybe. I-I, yeah. Yeah maybe. Also like I think these are like summaries so once you go to the actual reviews they might have more details. Yeah yeah um here let's uh let me share my screen are you looking at the other reviews? No Not right now. I've looked at them earlier though. Okay here um main comments: The paper lacks sufficient experiments in terms of computational tests and performance comparisons under the premise of fair comparison. Is there an existing DCP software package that can be directly compared with DGCP? If so, since the paper states that DGCP can verify a broader class of convexity, the authors could include an additional comparative experiment. This experiment should target convex problems that are both DCP and DGCP can verify and compare the capabilities in performing symbolic analysis and convexity verification in order to demonstrate whether DGCP I thought I did, like when it started out but then towards the end not really. Are that both DCP and DGCP can verify? I feel that's not what he means. I think he means problems that are only DGCP and not DCP. Like because how would we show a comparative comparative experimental if they are like both. We are not claiming that we improve DCP in any way. Right. We are just saying this is an additional thing. 
I don't understand this like whether it just DGCP achieved design level improvement? So I think Claude understood this as we need examples of DC problem that are DGCP but not DCP and then implemented that. I think that's what I have which like matches the first half of that statement this bullet but like that second part doesn't make any sense right. So target complex problems that are both DCP and DGCP? first of all that sentence is just ill formed I wonder if he's just like drunk or something when writing it and compare their capabilities in performing symbolic analysis in order to demonstrate whether DTCP or DGCP. This is just like a word soup it doesn't mean anything what the f*ck does this mean? Yeah I've no idea what this means. I think what I'm taking away is we can show experiments where oh like look at this convex dot j l or look at this cvx pi example that can't verify that this which on a non-convex problem says that it's non-convex but it's a geodesically convex problem and our library shows that it is geodesically convex. What do you think? Yeah I think how you interpret it is correct. Yeah this second sentence just doesn't make any sense to me. I don't know what design level improvements mean. Yeah I don't know either. I'm not feeling as motivated about this anymore, to be honest. Sorry just so irritating. The current numerical examples appear to focus primarily on SPD matrices and basic geometric tests. It is recommended to include more complex application cases So yeah just more examples I guess so for this maybe if at some point when writing the paper you came across problems that are more complex quote unquote we can just include that I think we can do a maximum likelihood estimation problem there are so not just the gaussian distribution but there are other distributions that are like fat tailed so they're actually pretty people use this in quant finance actually so like multivariate t distributions they have a fatter tail than gaussian distributions they're non-convex their mle is non-convex but they're but it's geodesically convex so we could write something up like that like there's a class of problems that are where maximum likelihood estimation is non-convex but geodesically convex So that should be complex enough, I hope. Let me just note this down. You're recording this right? Yeah, I'm recording our audio and then I'll feed it into something to get meeting notes. Okay let's see oh because I got another paper accepted with Melanie and that paper was literally what I just described here okay yeah sounds good sure or we could just even in that case you could just reference to that paper too like it's not just about toy problems we can figure it out either we show it or we just reference to it okay so the experiments in this paper concerning symbolic complexity and verification time remain insufficient symbolic complexity verification time could the author design one or more experiments to explore symbolic complexity and verification time in greater depth accompanied by reasonable analysis oh I guess like they want to see some discussion of how does the scale and so on like what we expect the complexity to be I think we can do that. I can, I can, we can, it's up to you like we can split this uh in some way. Let me also check I don't know what Claude did for this maybe so I mean it would be very easy to just do empirical experiments of timings and how it's scaling and so on and then maybe we can do some theoretical uh write up of what uh of how that will scale. I think I can do that. It's basically about like the tree depth of the expressions and so on shouldn't be too hard. I can, I can give this a shot. Okay 
All these, yeah, I'll address all the detail comments. I think that's okay, so these are just text changes and math changes, right? Yeah, these are all math definitions and ordering, I think looks like notation. That's not too bad, that's not too bad. Yeah, this one's not, yeah, damn okay, I thought it was the worst for some reason.
To be honest, I didn't actually read the reviews, I just gave them to Claude to read it and do the changes, dude. I don't know how I feel about GPT and Claude. It's really great for doing dumb shit like plotting and mundane things like summarizing notes, but for research, it's like I don't know, it's not that great, actually.
Bro, like people are solving our DOS problems with GPT 5.1, 5.2. Do you pay for stuff? First of all, are you using the pro account with Claude right now with GPT? Yeah, okay, whatever.
I'm surprised, I mean, yeah, I don't know what to tell you. They're pretty good to me, especially for code, like for writing code. Also, I'd say you need to unlock that habit of how you should be using it. Like, I use it for, uh, a lot of people in the company and so on in the world now are using it also for, like, the way we are talking to each other. When I don't know, like I struggle to, so maybe it's also like a person-to-person thing. Maybe you are very good at silent thinking and you are much better at verbal thinking.
But I like to verbalize things, especially in voice when I'm trying to think about things, so it just gives that way to have a dialogue and then refine on ideas. Sometimes I'll have this unstructured idea in my head, I'm not even sure what it's about. Let me try to give you an example. Maybe I think I understand what you mean. Yeah, yeah, so then having that, giving it, like, asking it to double down on it, asking it to ask me questions so that I think a little bit more about different parts of it and so on. Even that's been really, really useful for me in some ways. So I would expect in research, this part of it.
So let me give you just a couple of quick tips:
1. Ask it to ask you questions, I think that helps a lot
2. Ask it to iterate on its work
It's like I use my optimization hat a lot on this, especially with Claude Code because it's like this I know you're not using but maybe you're using Codex if you're using GPT pro. These coding agents if you ask them to iterate, they are basically these are just universal optimizers from what in my perspective, like, you know, these are just yeah, like, so if you ask them to iterate, like, just ask them to, so they did some work, ask them to now critique that work, then ask them to use that critique to, like, so on. Obviously, there's the danger of reward collapse with that because it's the same thing, so sometimes you want to start a fresh session and ask it to review the previous work that was done and so on, you have to be it's a muscle, I guess. Like, I just, that's all I do to be honest. My trust in these models is way higher than a lot of people just because I've been using them since I was at MIT. I started using Claude at MIT, it was just fucking knocking everything out of the park to be honest. So I developed that initial trust and never kind of had to go through those moments of doubt. But it's not that great at writing, for example. Every time I ask it to write something, I never feel happy. I have to spend so much more time writing it than if I would probably do if I just wrote it myself. But I actually feel for technical things, it is better than it is for, like, pure writing and so on. So yeah, I think GPT is amazing for learning, like the verbalization is really great. You just ask it to give you comprehension questions as you're learning. Yeah, I think that's super useful. WordCouples is like novel research that I'm doing in my
I'm like this is what I tried to accomplish and it gives me a couple of proofs and reiterates on this make sure it's correct, double-check, be comprehensive, and I start a new session. I do that like I understand the optimization person. Oh, okay, cool, and that's kind of how I do it anyways. But the problem is that it gives you something really confident, it looks really good, but hides key things that are really critically important. Sometimes I'm just not good enough to identify what it's hidden or the assumptions as they're made. I think yeah, with math, that becomes harder. With Core, it's like you just run that thing and you know whatever you can. It's easier to catch those things. But I mean, I don't think Terence Tau is using GPT for math, so yeah, like now we are not like I mean I'm not saying that you are saying that, but I mean anyone who says that "oh, I'm better than using AI" - I don't think that works anymore. Not that you are saying that, I'm just saying like in general. I think it's the opposite. I think it's because AI is just like I think I'm just my skill level is too close to AI because I feel like I would be much more proficient using it if I was a better mathematician. Right? Okay, it's like for Terence how he can look at what it generates and right away tell his or not, but I'm not that good yet. I get it like yeah it's kind of like garbage in, garbage out. How to try like I mean I feel that too. Very honestly, I like when I know some about something and I use that knowledge and get it to do something obviously it's like way better, but then I can just go vanilla, go do this and have no idea out of it. Like I will just trust whatever it gives me. So yeah, I completely get what you're saying. Yeah, like you got to be really good at what you're doing before using AI to help accelerate it. That's what I feel. Or then use it for learning and then yeah like become good at it. So yeah like maybe you can bootstrap yourself, but I don't. I just gotta be a lot better. I don't know how, but like oh I just gotta be a lot better than GPT. Like I just got because GPT is like what I think it's probably a top 95%. You got to be like the top 5% of your field to be able to meaningfully use GPT. I think that's my take. I agree. I agree. I guess I disagree a little bit, but I know what you're saying. I get it. I mostly agree. Okay, cool. All right, let's go through the last one. Yeah, and isn't the last one there? Aren't there three? This is the second one, right? Oh, yeah, my bad, but each of these is like two pages right? Not bad, this one sounds nice. 
So title, that's fine. For the optimization problem section five, since most of them are not Euclidean convex, but G-convex, I was trying to paper to demonstrate the benefits of DGCP by solving the problems as non-convex using state of the art local non-linear optimization solvers. And also with the Riemannian solver. Yeah, this is kind of what I was saying with one of the points in the previous review. Okay. So I guess this is something we could do together or I guess for the coding part you'd have to do it but we can discuss. So you can spend some time on this and if you have suggestions send them to me and then I'll also just implement some things and then we can combine everything we have used there. Okay. So for my part I guess I'll look through it, suggest ideas and then you could just suggest examples and problems. Okay. Then although section four provides a list of functions that demonstrate the correctness and effectiveness of Julia package it isn't clear whether the package might incorrectly assert geodesic convex for new functions? A benchmark is suggested to be established or collected to support such examination so it is unclear whether the package might incorrectly assert geodesic convex for new general functions. Is that true? I don't think so. No I don't think so either. Yeah no for sure. I guess they want us to show them examples of non-geodesically convex problems? Like maybe ones where we return false also instead of just returning true all the time in our paper. Yeah I guess they want us to have like a set of these things right like set of functions set up of functions that are you know convex but not geodesically convex and make sure everything it make sure it like spits out you know unknown or not not geodesically convex does that make sense which is like so many things that's an infinite infinite set right like how would we i just i guess like instead of like five six examples i don't know don't know also have to address everything right? Yeah I guess this is like not that big idea. What I'm taking away from it is we need to show some examples where we return false like that's what I'm I think. Okay yeah. We can iterate on it like yeah like sure exactly we don't need to address everything perfectly right now we can come back to it. Since the Julia package relies on symbolic expressions of non-linear functions and the authors note that DCP may fail for some convex functions, it suggests to add a discussion on the non-uniqueness of symbolic representations of the same function. For example, log x squared is not DCP valid while two log x's similar situations may occur for the proposed methodology and clarification of such limitations and possible remedies will be useful. So this is the canonicalization thing I was mentioning which I kind of have implemented already so I guess I don't discuss that in the paper anywhere so I can just surface that a bit. Okay could you highlight this and just say like "Well here I guess this is you can just make a note right?" Yeah can you how can I do it? You'll have to do it right? Yeah I'll do it and then I'll share this. Should we have a unified document like a Google Docs with all these points and then go through that? I think that's probably the best idea right? But we already did so much like we discussed so much. If you want to start that now or I thought you wanted to make comments on the PDF or something right? I can make comments on the PDF so I can start the Google Docs too. Yeah go ahead start the Google Doc and then. 
About TGCP and then okay, so I will rewrite, if you're able to introduce TGCP's definition in the beginning. Motivate why people should care. Okay, makes sense. The paper currently suffers from organizational issues that may obscure the rationale for applying the DCP framework. For instance, section 2.3 as a substantial portion of section 3.1 or devoted to discussions that resemble related work despite the presence of a dedicated subject and title related work. Okay, so this is just rewriting it. So I think I could do like a first pass, and then we could read it together to see if all the edits are good. Sound good? And incorporating there's a concern among them by having a comprehensive related section. Okay, so the paper lacks sufficient experiments in terms of computational tests and performance comparisons. So yeah, sorry, this is what we discussed before. So I'll add the log likelihood G convex from other paper. I think this is what you should do, right? Maybe you can add a comment. So yeah, let me do that. What are those? So okay. 
I'm looking at this part here. This is, I think, very reasonable, but maybe I don't want to say trivial, but it should be doable, right? Yeah, but this is a proof, so maybe you want to take it up. You're showing the author should explicitly demonstrate the correspondence between GCP and classical TCP assumption. Do you know what they mean by "explicitly demonstrate the correspondence between these two things?" Yeah, we kind of mention that DCP is, like, if we just change the manifold from SPD manifold to Euclidean manifold (with a flat metric, I don't know, the regular Euclidean metric) they correspond, right? That's what we kind of claim in the paper, so I think they want us to prove that. Can you repeat that? So, like, our DGCP is the SPD manifold with that specific metric, right? But if you change that to be Euclidean manifold with the Euclidean metric, the two non-metric that then becomes DCP, right? Yeah, okay, did that make sense now? Like, I just think it's kind of trivial, in that, yeah, I agree, what I didn't want to say trivial, but maybe just a couple of lines of math to show. I guess I don't even understand what they're confused about. In the sense that, like, you're just, "Oh, I guess one thing to make this super explicit is the notion of convexity vs geodesic convexity," like the definition, right? Because, essentially, geodesic convexity is replacing straight lines as being shortest distance paths between two points with the geodesic, right? That's the only difference, yep, and it gives rise to these different convexity behaviors depending on the notion, and that's, I think, that's better in terms of addressing this point. Cool. Which I think we already have in the paper, maybe we just need an explicit clarification to explain how we claim this, right? By replacing these metrics and so on, the difference between G-convexity vs Euclidean convexity, so geodesics vs chords and how it gives rise to different convexity behaviors on a fixed set. Okay, sounds good. Number five: experimental evaluation presented in the paper appears somewhat limited, typically expected by MPC, specifically Section 4.4 focuses exclusively on verification time for small to moderate scale problems, all evaluated on a single machine. To better situate this work within the broader context, and demonstrate its practical utility, the following aspects could be strengthened: demonstration of non-GC identification, it would be valuable if the authors could provide explicit examples using key atoms to illustrate how the framework recognizes functions that are not G-convex. Yeah, I mean, this is a repeated comment, but yeah, we'll do that, okay. Comparison with expert-driven analysis, yeah, so for this, when I read this, I remember, I actually read this one, isn't there the square root paper from Suvrit? That's a pretty long paper, and then we were able to just do that, pretty easily with our principles, and I remember you proved it out on paper using our principles, and then we looked at how is over it did it, which was, like, a shit ton of work, am I right? Or was it some other proof? I'm sure there was one proof from Suvrit that was, like, multiple pages, and then when you tried to do it with DGCP, it was kind of just trivial or, like, just a couple of lines. No, I think that's a good point, yeah, I can look at this, yeah, this is also existing literature, look at problems where complex. 
I don't know how to handle this. Can the given importance of geodesic and structured manifold optimization algorithms in the DGCP framework effectively characterize or leverage information about geodesic in this verification? It doesn't write like, "You first choose a geodesic. And then we choose a geodesic, we fix that geodesic, then that gives rise to the atoms and functions or rules based off that geodesic." So I can't adapt it to geodesic. You have to build out a framework manually, right? Yeah. So maybe we won't address that. Okay, so maybe we'll say, "The response given a fixed geodesic, we need to manually build out the atoms and the rules. We can't have an adaptive procedure that effectively leverages information about geodesic and its verification process." I mean, what we can do is like, I guess, in your framework, you choose the geodesic, right? Like, I guess, we've had off at least two geodesics in our DGCP paper, we have the convex one and then, I guess, at least two geodesics, right? We have the Euclidean one and we have this Riemannian one. How do you toggle between the two? You mean in the software, so you have to specify the geodesic you're using when you set up the problem, which automatically determines which set of atoms and rules to use. Sorry, alright, so that's how we're going to respond to that. So I agree with the author. However, other languages in Python, MATLAB, or the Poplern optimization community. Therefore, can that author implement DGCP? Oh, this is what you're doing right? No, I want to say no to it because the thing is, it will be easy to say that, "Yeah, I can go implement it," but then I'll have to spend a significant amount of time debugging and like, "I don't you I thought you threw it into Claude or something." I said, "I can do it. I didn't do it here." It might be worth trying once, I guess. Yeah, no, probably, yeah, I mean, if you don't have time, we can just tell how they may, we can just tell the people how they can do it. We don't need to do it. Go ask Claude Coat to do it. Is that all? Like, that's all I can say, tell the readers how to do it. Oh, I guess, like, we can say that the CVXPy community can, yeah, someone can walk with the CVXPy community to improve extend their, I guess, maybe you could if you want, if it makes sense. After we have all our changes, we can address this point in future work, right? Yeah, yeah, we can tell you in future work we'll address this comment in future work. Since Vaibhav has moved on from academia, as soon as another cheap labor comes along, this can be done. But like, right now, alright, all of these are fine, this is all okay. 
